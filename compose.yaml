services:
  # Kafka broker using KRaft mode
  kafka:
    image: apache/kafka:3.8.0  # Pin specific version instead of latest
    container_name: kafka-broker
    ports:
      - "9092:9092"
    environment:
      # Configure listeners for both docker and host communication
      KAFKA_LISTENERS: CONTROLLER://kafka:9091,HOST://0.0.0.0:9092,DOCKER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: HOST://localhost:9092,DOCKER://${KAFKA_BOOTSTRAP_SERVER}
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,DOCKER:PLAINTEXT,HOST:PLAINTEXT

      # Settings required for KRaft mode
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9091

      # Listener to use for broker-to-broker communication
      KAFKA_INTER_BROKER_LISTENER_NAME: DOCKER

      # Required for a single node cluster
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

      # Storage configuration
      KAFKA_LOG_DIRS: /tmp/kraft-combined-logs
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk

    volumes:
      - kafka-data:/var/lib/kafka/data # Persist Kafka data
    networks:
      - common-network
    healthcheck:
      test: [ "CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list || exit 1" ]
      interval: 10s
      timeout: 10s
      retries: 15
      start_period: 30s

  # Kafka UI for managing and monitoring Kafka
  kafka-ui:
    image: kafbat/kafka-ui:main
    container_name: kafka-ui
    ports:
      - 8080:8080
    environment:
      DYNAMIC_CONFIG_ENABLED: "true"
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9093
    depends_on:
      kafka:
        condition: service_healthy  # Wait for broker to be healthy
    networks:
      - common-network
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:8080/ || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  
   # Kafka Connect with MongoDB Sink plugin
  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.6.1
    container_name: kafka-connect
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8083:8083"  # Kafka Connect REST API
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9093
      CONNECT_GROUP_ID: connect-cluster
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_PLUGIN_PATH: /usr/share/java,/etc/kafka-connect/jars
      # Install MongoDB Kafka Connector
      CONNECTOR_PLUGINS: mongodb
    volumes:
      - ./kafka/connect/plugins:/etc/kafka-connect/jars
      - ./kafka/connect/mongodb-sink.json:/configs/mongodb-sink.json:ro
    networks:
      - common-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8083/connectors || exit 1" ]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 40s

  # MongoDB database
  mongo:
    image: mongo:7
    container_name: mongo
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
    networks:
      - common-network
    healthcheck:
      test: ["CMD", "mongosh", "--quiet", "--eval", "db.runCommand({ ping: 1 })"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  # Mongo Express UI for MongoDB (development only)
  mongo-express:
    image: mongo-express:1.0.2
    container_name: mongo-express
    restart: unless-stopped
    ports:
      - "8099:8081"   # Web UI at http://localhost:8099
    environment:
      ME_CONFIG_MONGODB_URL: mongodb://mongo:27017
      ME_CONFIG_BASICAUTH_USERNAME: "admin"
      ME_CONFIG_BASICAUTH_PASSWORD: "admin"
      ME_CONFIG_OPTIONS_EDITORTHEME: ambiance
    depends_on:
      mongo:
        condition: service_healthy
    networks:
      - common-network
  
  # PostgreSQL database for storing data
  postgres:
    image: postgres
    container_name: postgres-db
    restart: always
    # set shared memory limit when using docker compose
    shm_size: 128mb
    # or set shared memory limit when deploy via swarm stack
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_HOST_AUTH_METHOD: trust # Allow connections without password for local development
      POSTGRES_PORT: 5432  # Specify the port for PostgreSQL
      TZ: Europe/Rome
    volumes:
      - type: tmpfs
        target: /dev/shm
        tmpfs:
          size: 134217728 # 128*2^20 bytes = 128Mb

      - postgres_data:/var/lib/postgresql/data
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql # Initialize database with SQL script and postgres will run it
      - ./postgres/init-cdc.sh:/docker-entrypoint-initdb.d/init-cdc.sh # CDC configuration script
      - ./postgres/seed.sql:/docker-entrypoint-initdb.d/seed.sql # Seed data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - common-network

  # Adminer for database management
  adminer:
    image: adminer
    container_name: adminer
    restart: always
    ports:
      - 8050:8080
    depends_on:
      postgres:
        condition: service_healthy  # Wait for PostgreSQL to be healthy
    networks:
      - common-network

  # Data API service for collecting market data
  market-data-api:
    image: abiget/market-data-api:0.1
    container_name: market-data-collection-api
    platform: linux/amd64
    ports:
      - "${DATA_API_PORT}:8000"
    environment:
      TZ: Europe/Rome
    depends_on:
      # Only depends on itself to start after infrastructure is ready
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
    networks:
      - common-network
    healthcheck:
      test: ["CMD", "curl", "-f", "${DATA_API_URL}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    volumes:
      - ./apis:/app  # For development - live reload


  # Flink job manager and task manager for processing data
  flink-jobmanager:
    image: abiget/market-flink:0.1
    container_name: flink-jobmanager
    platform: linux/amd64
    ports:
      - "8081:8081"  # Flink Web UI
    command: jobmanager
    # standalone-job --python /opt/flink/processing/src/jobs/price_analyzer.py
    depends_on:
      - kafka
      - postgres
      - market-data-producer
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager

        jobmanager.memory.process.size: 1800m
        jobmanager.memory.jvm-overhead.min: 192m

        state.backend: filesystem
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        heartbeat.interval: 1000
        heartbeat.timeout: 5000
        rest.flamegraph.enabled: true
        web.backpressure.refresh-interval: 10000
        execution.shutdown-on-application-finish: false
      - FLINK_LOCAL_TIME_ZONE=Europe/Rome
      - TZ=Europe/Rome
      - ALERTS_KAFKA_STARTUP_MODE=latest-offset
      - UNDERCUT_PERCENT_THRESHOLD=${UNDERCUT_PERCENT_THRESHOLD}
      - OVERPRICED_PERCENT_THRESHOLD=${OVERPRICED_PERCENT_THRESHOLD}
      - PRICE_INCREASE_24HRS_THRESHOLD=${PRICE_INCREASE_24HRS_THRESHOLD}
      - SEQUENTIAL_CHANGE_THRESHOLD=${SEQUENTIAL_CHANGE_THRESHOLD}
      - KAFKA_TOPIC=${COMPETITOR_PRICES}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/overview"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    volumes:
      - ./flink:/opt/flink/app
      - flink_data:/tmp/
    networks:
      - common-network
  flink-taskmanager:
    image: abiget/market-flink:0.1
    container_name: flink-taskmanager
    platform: linux/amd64
    command: taskmanager
    depends_on:
      - flink-jobmanager
      - kafka
      - postgres
      - market-data-producer
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.memory.process.size: 3072m
        taskmanager.numberOfTaskSlots: 8
        state.backend: filesystem
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        heartbeat.interval: 1000
        heartbeat.timeout: 5000
      - FLINK_LOCAL_TIME_ZONE=Europe/Rome
      - TZ=Europe/Rome
    volumes:
      - flink_data:/tmp/
    networks:
      - common-network

  # Market data Kafka producer
  market-data-producer:
    image: abiget/market-data-producer:0.1
    container_name: market-data-producer
    platform: linux/amd64
    depends_on:
      kafka:
        condition: service_healthy
      market-data-api:
        condition: service_healthy
    environment:
      - KAFKA_BOOTSTRAP_SERVER=${KAFKA_BOOTSTRAP_SERVER}
      - DATA_API_INTERNAL_URL=${DATA_API_INTERNAL_URL}
      - COLLECTION_INTERVAL=${COLLECTION_INTERVAL}
    command: ["python", "producers/market_data_producer.py"]
    volumes:
      - ./kafka:/kafka  # live-reload code for development
    networks:
      - common-network

  # User behavior producer
  user-behavior-producer:
    image: abiget/market-data-producer:0.1
    container_name: user-behavior-producer
    platform: linux/amd64
    depends_on:
      kafka:
        condition: service_healthy
      market-data-api:
        condition: service_healthy
    environment:
      - KAFKA_BOOTSTRAP_SERVER=${KAFKA_BOOTSTRAP_SERVER}
      - DATA_API_INTERNAL_URL=${DATA_API_INTERNAL_URL}
      - USER_BEHAVIOR_INTERVAL=${USER_BEHAVIOR_INTERVAL}
    command: ["python", "producers/user_behavior_producer.py"]
    volumes:
      - ./kafka:/kafka
    networks:
      - common-network
  # Dashboard for price intelligence visualization
  dashboard:
    image: abiget/price-intelligence-dashboard:0.1
    container_name: price-intelligence-dashboard
    platform: linux/amd64
    ports:
      - "8501:8501"
    environment:
      - POSTGRES_URL=${POSTGRES_URL}
      - ALERT_PERCENT_THRESHOLD=${ALERT_PERCENT_THRESHOLD}
      - UNDERCUT_PERCENT_THRESHOLD=${UNDERCUT_PERCENT_THRESHOLD}
      - OVERPRICED_PERCENT_THRESHOLD=${OVERPRICED_PERCENT_THRESHOLD}
      - PRICE_INCREASE_24HRS_THRESHOLD=${PRICE_INCREASE_24HRS_THRESHOLD}
      - SEQUENTIAL_CHANGE_THRESHOLD=${SEQUENTIAL_CHANGE_THRESHOLD}
    depends_on:
      - postgres
    volumes:
      - ./dashboard:/app
    networks:
      - common-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Price sensitivity updater (ml-based elasticity)
  price-sensitivity-updater:
    image: abiget/price-sensitivity-updater:0.2
    container_name: price-sensitivity-updater
    platform: linux/amd64
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PORT=${POSTGRES_PORT}
      - PRICE_SENSITIVITY_REFRESH_SEC=${PRICE_SENSITIVITY_REFRESH_SEC}
      - DATA_API_INTERNAL_URL=${DATA_API_INTERNAL_URL}
      - MIN_ELASTICITY_OBSERVATIONS=${MIN_ELASTICITY_OBSERVATIONS}
      - PRICE_DEMAND_OBS_LOOKBACK_HOURS=${PRICE_DEMAND_OBS_LOOKBACK_HOURS}
      - ELASTICITY_ROLLUP_MINUTES=${ELASTICITY_ROLLUP_MINUTES}
      - ELASTICITY_HIGH_THRESHOLD=${ELASTICITY_HIGH_THRESHOLD}
      - ELASTICITY_MEDIUM_THRESHOLD=${ELASTICITY_MEDIUM_THRESHOLD}
    depends_on:
      kafka:
        condition: service_healthy
      market-data-api:
        condition: service_healthy
      postgres:
        condition: service_healthy
      flink-jobmanager:
        condition: service_healthy
    command: ["python", "price_sensitivity_updater.py"]
    volumes:
      - ./ml_services/price_elasticity:/price_elasticity
    networks:
      - common-network

  # Price adjustment service
  price-adjustment:
    # build:
    #   context: ./ml_services/price_adjustment
    #   dockerfile: Dockerfile
    image: abiget/price-adjuster:0.1
    container_name: price-adjustment
    platform: linux/amd64
    environment:
      - PRICE_GRID_PCT=${PRICE_GRID_PCT}
      - MIN_MARGIN_PCT=${MIN_MARGIN_PCT}
      - DASK_WORKERS=${DASK_WORKERS}
      - DASK_DASHBOARD=${DASK_DASHBOARD}
      - TARGET_UNDERCUT_PCT=${TARGET_UNDERCUT_PCT}
      - GAP_TOLERANCE_PCT=${GAP_TOLERANCE_PCT}
      - FORECAST_MIN_CONF=${FORECAST_MIN_CONF}
      - RUN_INTERVAL_SECONDS=${RUN_INTERVAL_SECONDS}
      - AUTO_APPLY_PENDING=${AUTO_APPLY_PENDING}
      - APPLY_LIMIT=${APPLY_LIMIT}
    depends_on:
      - postgres
      - flink-jobmanager
      - kafka
      - market-data-api
      - price-sensitivity-updater
    volumes:
      - ./ml_services/price_adjustment:/app
    command: ["sh", "-c", "while true; do python price_adjustment.py && python apply_proposals.py; sleep ${RUN_INTERVAL_SECONDS:-300}; done"]
    restart: unless-stopped
    networks:
      - common-network

volumes:
  kafka-data:
  mongo_data:
  postgres_data:
  flink_data:
networks:
  common-network: